{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from converters import SensorThings2Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5632\n"
     ]
    }
   ],
   "source": [
    "with open(\"ABU1.txt\") as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        features = SensorThings2Dict(line)\n",
    "        data.append(list(features.values()))\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Total: 3934 Good: 3718 Faulty: 216 Ratio: 0.0549059481444\n",
      "Test  Total: 1698 Good: 1601 Faulty: 97 Ratio: 0.0246568378241\n"
     ]
    }
   ],
   "source": [
    "data = np.asarray(data)\n",
    "#print(data[:,2:-1])\n",
    "numerics = data[:,2:-1]\n",
    "# for n in numerics:\n",
    "#     plt.plot(n);\n",
    "mask = np.random.rand(len(data)) < 0.7\n",
    "train = data[mask]\n",
    "test = data[~mask]\n",
    "print(\"Train Total: {} Good: {} Faulty: {} Ratio: {}\".format(len(train), len(train[train[:,-1]=='True']), len(train[train[:,-1]=='False']), float(len(train[train[:,-1]=='False']))/len(train)))\n",
    "print(\"Test  Total: {} Good: {} Faulty: {} Ratio: {}\".format(len(test), len(test[test[:,-1]=='True']), len(test[test[:,-1]=='False']), float(len(test[test[:,-1]=='False']))/len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Total: 5878 Good: 3718 Faulty: 2160 Ratio: 0.367471929228\n",
      "(5878L, 54L)\n"
     ]
    }
   ],
   "source": [
    "faulty = train[train[:,-1]=='False']\n",
    "not_faulty = train[train[:,-1]=='True']\n",
    "train = np.concatenate((not_faulty, np.repeat(faulty, 10, axis=0))) # repeate faulties\n",
    "#train = np.concatenate((not_faulty[:len(faulty)], faulty))\n",
    "print(\"Train Total: {} Good: {} Faulty: {} Ratio: {}\".format(len(train), len(train[train[:,-1]=='True']), len(train[train[:,-1]=='False']), float(len(train[train[:,-1]=='False']))/len(train)))\n",
    "train = train[np.random.permutation(train.shape[0])] # shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train[:,2:-1].astype(np.float32)\n",
    "train_labels = np.array(train[:,-1]=='False').astype(np.int32)\n",
    "test_data = test[:,2:-1].astype(np.float32)\n",
    "test_labels = np.array(test[:,-1]=='False').astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(expected, predicted):\n",
    "    print(\"------------------------- EVALUATION -------------------------\")\n",
    "    print(\"Accuracy Score: {}\".format(metrics.accuracy_score(expected, predicted)))\n",
    "    print(metrics.classification_report(expected, predicted))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(expected, predicted))\n",
    "    print(\"Kappa Score: {}\".format(metrics.cohen_kappa_score(expected, predicted)))\n",
    "    print(\"Matthews Correlation Coefficient: {}\".format(metrics.matthews_corrcoef(expected, predicted)))\n",
    "    print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00      2160\n",
      "       True       1.00      1.00      1.00      3718\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2160    0]\n",
      " [   0 3718]]\n",
      "Kappa Score: 1.0\n",
      "Matthews Correlation Coefficient: 1.0\n",
      "--------------------------------------------------------------\n",
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 0.931684334511\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.40      0.38      0.39        97\n",
      "       True       0.96      0.97      0.96      1601\n",
      "\n",
      "avg / total       0.93      0.93      0.93      1698\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  37   60]\n",
      " [  56 1545]]\n",
      "Kappa Score: 0.353308512105\n",
      "Matthews Correlation Coefficient: 0.353396390964\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CART Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "p = clf.predict(train_data)\n",
    "pt = clf.predict(test_data)\n",
    "\n",
    "eval_metrics(train_labels, p)\n",
    "eval_metrics(test_labels, pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00      2160\n",
      "       True       1.00      1.00      1.00      3718\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2160    0]\n",
      " [   0 3718]]\n",
      "Kappa Score: 1.0\n",
      "Matthews Correlation Coefficient: 1.0\n",
      "--------------------------------------------------------------\n",
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 0.945229681979\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      0.06      0.11        97\n",
      "       True       0.95      1.00      0.97      1601\n",
      "\n",
      "avg / total       0.93      0.95      0.92      1698\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   6   91]\n",
      " [   2 1599]]\n",
      "Kappa Score: 0.106507938304\n",
      "Matthews Correlation Coefficient: 0.205405486469\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(train_data, train_labels)\n",
    "\n",
    "p = clf.predict(train_data)\n",
    "pt = clf.predict(test_data)\n",
    "\n",
    "eval_metrics(train_labels, p)\n",
    "eval_metrics(test_labels, pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 0.647329023477\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.59      0.13      0.22      2160\n",
      "       True       0.65      0.95      0.77      3718\n",
      "\n",
      "avg / total       0.63      0.65      0.57      5878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 290 1870]\n",
      " [ 203 3515]]\n",
      "Kappa Score: 0.0950256577\n",
      "Matthews Correlation Coefficient: 0.138549711009\n",
      "--------------------------------------------------------------\n",
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 0.891637220259\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.07      0.07      0.07        97\n",
      "       True       0.94      0.94      0.94      1601\n",
      "\n",
      "avg / total       0.89      0.89      0.89      1698\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   7   90]\n",
      " [  94 1507]]\n",
      "Kappa Score: 0.0131960456082\n",
      "Matthews Correlation Coefficient: 0.0131990830631\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(25,50,25), random_state=1, \n",
    "                    max_iter=1000, verbose=True, tol=1e-6)\n",
    "clf = clf.fit(train_data, train_labels)\n",
    "\n",
    "p = clf.predict(train_data)\n",
    "pt = clf.predict(test_data)\n",
    "\n",
    "eval_metrics(train_labels, p)\n",
    "eval_metrics(test_labels, pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 0.982647158898\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      1.00      0.98      2160\n",
      "       True       1.00      0.98      0.99      3718\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2150   10]\n",
      " [  92 3626]]\n",
      "Kappa Score: 0.96296640369\n",
      "Matthews Correlation Coefficient: 0.963393463897\n",
      "--------------------------------------------------------------\n",
      "------------------------- EVALUATION -------------------------\n",
      "Accuracy Score: 0.901060070671\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.05      0.04      0.05        97\n",
      "       True       0.94      0.95      0.95      1601\n",
      "\n",
      "avg / total       0.89      0.90      0.90      1698\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   4   93]\n",
      " [  75 1526]]\n",
      "Kappa Score: -0.00614414401704\n",
      "Matthews Correlation Coefficient: -0.0061801590477\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "p = clf.predict(train_data)\n",
    "pt = clf.predict(test_data)\n",
    "\n",
    "eval_metrics(train_labels, p)\n",
    "eval_metrics(test_labels, pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67         2\n",
      "          1       0.75      1.00      0.86         3\n",
      "\n",
      "avg / total       0.85      0.80      0.78         5\n",
      "\n",
      "[[1 1]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "expected = [1,1,1,0,0]\n",
    "predicted = [1,1,1,1,0]\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
